);
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="glm", trControl=kFoldControl);
predictionSet <-summaryTable$bugCovering;
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="glm", trControl=kFoldControl);
predictionSet <-as.factor(trainingData$bugCovering);
predictionSet <-as.factor(summaryTable$bugCovering);
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="glm", trControl=kFoldControl);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
explanatorySet<-summaryTable$rankingVote;
predictionSet <-as.factor(summaryTable$bugCovering);
myFolds <- createFolds(explanatorySet, k = 10)
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="glm", trControl=kFoldControl);
explanatorySet<-as.numeric(summaryTable$rankingVote);
predictionSet <-as.factor(summaryTable$bugCovering);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact
#same datasets for all models
myFolds <- createFolds(explanatorySet, k = 10)
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="glm", trControl=kFoldControl);
View(summaryTable)
explanatorySet
predictionSet
GLM_model<- train(predictionSet ~ explanatorySet,summaryTable, method="knn", trControl=kFoldControl);
?make.names
myFolds <- createFolds(summaryTable$rankingVote, k = 10)
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(summaryTable$bugCovering ~ summaryTable$rankingVote,summaryTable, method="glm", trControl=kFoldControl);
summaryTable$bugCovering <-as.factor(summaryTable$bugCovering);
GLM_model<- train(summaryTable$bugCovering ~ summaryTable$rankingVote,summaryTable, method="glm", trControl=kFoldControl);
View(summaryTable)
View(summaryTable)
summaryTable$rankingVote<-as.numeric(summaryTable$rankingVote);
summaryTable$bugCovering <-as.factor(summaryTable$bugCovering);
myFolds <- createFolds(summaryTable$rankingVote, k = 10)
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(summaryTable$bugCovering ~ summaryTable$rankingVote,summaryTable, method="glm", trControl=kFoldControl);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets. One way to do that is
#to shuffle the data and sample from it.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Import data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
install.packages(ElemStatLearn)
library(ElemStatLearn)
library(klaR)
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
nb.fit = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=5))
xTrain = summaryTable[,"rankingVote"]
yTrain = as.factor(summaryTable$bugCovering);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
xTrain = summaryTable[,"rankingVote"]
yTrain = as.factor(summaryTable$bugCovering);
myFolds <- createFolds(xTrain, k = 10)
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(yTrain ~ xTrain,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(y=yTrain, x=xTrain,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(y=bugCovering, x=ranking,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(y=bugCovering, x=rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(y=bugCovering, x=rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(yTrain, xTrain,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(xTrain, yTrain,summaryTable, method="glm", trControl=kFoldControl);
#convert columns to numeric
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
xTrain = summaryTable[,"rankingVote"]
yTrain = as.factor(summaryTable$bugCovering);
myFolds <- createFolds(xTrain, k = 10)
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(xTrain, yTrain,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(xTrain, yTrain, method="glm", trControl=kFoldControl);
GLM_model<- train(xTrain, yTrain,  trControl=kFoldControl);
GLM_model<- train(xTrain, yTrain, method="kn", trControl=kFoldControl);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
nb.fit = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=5))
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
xTest = test[,"rankingVote"]
yTest = as.factor(test$bugCovering);
xS = summaryTable[,"rankingVote"]
yS = data.frame(summaryTable[,"bugCovering"]);
nb.fit = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=5))
# Import data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
totalData = length(summaryTable$Question.ID);
trainingSize = trunc(totalData * 0.7);
startTestIndex = trainingSize + 1;
endTestIndex = totalData;
#convert columns to numeric
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
xTest = test[,"rankingVote"]
yTest = as.factor(test$bugCovering);
xS = summaryTable[,"rankingVote"]
yS = data.frame(summaryTable[,"bugCovering"]);
nb.fit = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=5))
install.packages(ElemStatLearn)
library(ElemStatLearn)
library(klaR)
library(caret)
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
xTest = test[,"rankingVote"]
yTest = as.factor(test$bugCovering);
xS = summaryTable[,"rankingVote"]
yS = data.frame(summaryTable[,"bugCovering"]);
nb.fit = train(xTrain,yTrain,'nb',trControl=trainControl(method='cv',number=5))
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
sub = sample(nrow(summaryTable), floor(nrow(summaryTable) * 1))
train = summaryTable[sub,]
test = summaryTable[-sub,]
xTrain = train[,"rankingVote"]
yTrain = as.factor(train$bugCovering);
nb.fit = train(bugCovering,rankingVote,summaryTable,'nb',trControl=trainControl(method='cv',number=5))
nb.fit = train(bugCovering~rankingVote,summaryTable,'nb',trControl=trainControl(method='cv',number=5))
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="kn", trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="knn", trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="nb", trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="nb", trControl=trainControl(method='cv',number=5));#trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=trainControl(method='cv',number=5));#trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary,
)
myFolds <- createFolds(xTrain, k = 10)
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
myFolds <- createFolds(summaryTable[,"rankingVote"] , k = 10)
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE,
# savePredictions = TRUE,
#  summaryFunction = twoClassSummary,
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
#classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
#  summaryFunction = twoClassSummary,
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
#classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary,
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
#classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCovering <- as.factor(summaryTable$bugCovering);
nb.fit = train(bugCovering ~ rankingVote,summaryTable,'nb',trControl=trainControl(method='cv',number=5))
trControl=trainControl(
method='cv',
number=5,
classProbs = TRUE # IMPORTANT!
);
trControl=trainControl(
method='cv',
number=5,
classProbs = TRUE # IMPORTANT!
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=trControl);
trControl=trainControl(
method='cv',
number=5
# classProbs = TRUE # IMPORTANT!
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=trControl);
trControl=trainControl(
method='cv',
number=5,
# classProbs = TRUE # IMPORTANT!
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=trControl);
?trainControl
trControl=trainControl(
method='cv',
number=5,
# classProbs = TRUE # IMPORTANT!
summaryFunction = twoClassSummary
);
trControl=trainControl(
method='cv',
number=5,
classProbs = TRUE, # IMPORTANT!
savePredictions = TRUE,
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=trControl);
summaryTable$bugCoveringLabels <- as.factor(summaryTable$bugCovering);
?replace
replace(summaryTable$bugCoveringLabels,bugCoveringLabels==FALSE,nonBuggy);
replace(summaryTable$bugCoveringLabels,bugCoveringLabels==FALSE,"notFaulty");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE,"notFaulty");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE,1);
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCoveringLabels <- as.factor(summaryTable$bugCovering);
summaryTable<- data.frame(summaryTable)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCoveringLabels <- as.factor(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE,1);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",1);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE","1");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor("1"));
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor(1));
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor(A));
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor("A"));
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCoveringLabels <- as.factor(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor("A"));
summaryTable$bugCoveringLabels <- summaryTable$bugCovering;
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",as.factor("A"));
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE",1);
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable$bugCoveringLabels <- summaryTable$bugCovering;
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 1);
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-1;
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-"A";
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-FALSE;
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 1);
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-"FALSEE";
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-as.factor("FALSEE");
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-as.character("FALSEE");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 1);
summaryTable$bugCoveringLabels[is.na(summaryTable$bugCoveringLabels)]<-as.character("FALSEE");
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 1);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==TRUE, 1);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==TRUE, 1);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels==FALSE, 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", 1);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", 1);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", 1);
replace(summaryTable$bugCoveringLabels,"FALSE", 0);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", 1);
replace(summaryTable$bugCoveringLabels,"FALSE", 0);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
nb.fit = train(bugCoveringLabels ~ rankingVote,summaryTable,'nb',trControl=trainControl(method='cv',number=5))
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
nb.fit = train(bugCoveringLabels ~ rankingVote,summaryTable,'nb',trControl=trainControl(method='cv',number=5))
kFoldControl <- trainControl(
index = myFolds,
number=5,
# classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
#summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
View(summaryTable)
View(summaryTable)
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact
#same datasets for all models
myFolds <- createFolds(summaryTable[,"rankingVote"] , k = 10)
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds,
classProbs = TRUE, # IMPORTANT!
# verboseIter = TRUE,
# savePredictions = TRUE,
summaryFunction = twoClassSummary
);
GLM_model<- train(bugCovering ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model<- train(bugCoveringLabels ~ rankingVote,summaryTable, method="glm", trControl=kFoldControl);
GLM_model
