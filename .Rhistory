#Load utility functions
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_SelfHealingUtility//loadData.R");
library(xgboost)
library(r2pmml) #https://github.com/jpmml/r2pmml
a <- {1.4, 2.5, 3.1, 5.5}
a <- c(1.4, 2.5, 3.1, 5.5)
b <- c(2.1, 3.9, 4.3, 6.7)
sd(a)
sd(b)
1/2*100
100 * 1/2
100 * 2/3
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
#Download and install the latest H2O package for R.
install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))
library(h2o)
h2o.init()
demo(h2o.kmeans)
install.packages("gbm")
install.packages("devtools")
install_git("git://github.com/jpmml/r2pmml.git")
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
mcResultsf <- data.frame(matrix(data=NA,nrow=3,ncol=8));
colnames(mcResultsf) <- c("DataSet","Train_RMSE_MEAN","Train_RMSE_STD","Test_RMSE_MEAN",
"Test_RMSE_STD","RMSE","R_Squared", "MAPD");
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
modelList <- c("Linear","Discontinuous","Saturating","ALL");
modelName <- modelList[1];
datasetSize <- c("1K","3K","9K");
datasetName <- generateDataSetNames(modelName,datasetSize,0);
# Generate the dataset names that will be trained ----------------------------
generateDataSetNames <- function(modelName,datasetSize,s_idx){
if(s_idx==0 & length(datasetSize)>0){#Generate for all sizes
datasetName <- paste0(modelName,datasetSize[1]);
for(i in c(2:length(datasetSize))){
datasetName <- cbind(datasetName,paste0(modelName,datasetSize[i]));
}
}
else{
datasetName <- paste0(modelName,datasetSize[s_idx]);
}
return(datasetName);
}
# Save results to file ----------------------------------------------------
resultsToFile <- function(mcResults,modelName,extension){
fileName <- paste0("mcResultsf_",modelName,extension);
write.table(mcResults,fileName,sep=",",col.names = TRUE);
print(paste0("file written:",fileName));
mcResults
}
# Prepare features --------------------------------------------------------
prepareFeatures <- function(dataf,selectionType){
#Do feature selection (or not)
if(selectionType=="ALL")
featuresdf<- select_ALL(dataf)
else
if(selectionType=="Linear")
featuresdf<- select_Linear(dataf)
else
if(selectionType=="Discontinuous")
featuresdf<- select_Discontinuous(dataf)
else
if(selectionType=="Saturating")
featuresdf<- select_Saturation(dataf)
#Remove zero utilities
featuresdf <- featuresdf[featuresdf$UTILITY_INCREASE!=0,];
# Scramble data
featuresdf <- scrambleData(datadf=featuresdf);
return (featuresdf);
}
library(devtools)
library(xgboost)
library(lightgbm, quietly=TRUE)
library(r2pmml) #https://github.com/jpmml/r2pmml
library(lgbm)
devtools::install_github("Laurae2/lgbdl", force = TRUE)
lgb.dl(commit = "master",
compiler = "vs",
repo = "https://github.com/Microsoft/LightGBM")
library(lightgbm)
install_github("Microsoft/LightGBM", subdir = "R-package")
install.packages("devtools")
library(devtools)
install_github("Microsoft/LightGBM", subdir = "R-package")
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
install_github("Microsoft/LightGBM", subdir = "R-package")
library(lightgbm)
data(agaricus.train, package='lightgbm')
train <- agaricus.train
dtrain <- lgb.Dataset(train$data, label=train$label)
params <- list(objective="regression", metric="l2")
model <- lgb.cv(params, dtrain, 10, nfold=5, min_data=1, learning_rate=1, early_stopping_rounds=10)
## Control code to run Random Fores
#https://machinelearningmastery.com/tune-machine-learning-algorithms-in-r/
library(randomForest)
library(caret)
library(devtools)
library(r2pmml) #https://github.com/jpmml/r2pmml
# Initialization section ------------------------------------------------------
#Load utility functions
source("C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//loadData.R");
#Data structure to keep results
#Folder with training data
folder <- "C://Users//Chris//Documents//GitHub//ML_SelfHealingUtility//data//DataPoints_1K-3K-9K//";
#folder <- "//DataPoints_1K-3K-9K//";
model.name.list <- c("Linear","Discontinuous","Saturating","ALL");
model.name <- model.name.list[2];
method.name <- "RF";
dataset.name.list <- generateDataSetNames(model.name, c("1K","2K","9K"),3);
results.df <- data.frame(matrix(data=NA,nrow=1000,ncol=14));
colnames(results.df) <- c("Item","Utility_Type","RMSE","R_Squared", "MADP","User_Time","Sys_Time","Elapsed_Time",
"Number_of_Trees","Learning_Rate","Max_Depth","Train_Split","Min_Data_In_Leaf","Bagging_Fraction");
results_line <- 0;
library(caret)
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#Split data between training and validation
#Extract training and validation sets
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
ibrary(caret)
# Import data -------------------------------------------------------------
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Convert columns to numeric ----------------------------------------------
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Split data for training and validating ----------------------------------
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
# Create trainControl to be reused by all models --------------------------
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(training.df[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
install.packages("KlaR")
install.packages("klaR")
library(klaR)
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
install.packages("caret")
install.packages("klaR")
library(klaR)
library(caret)
install.packages("caret")
install.packages("caret")
install.packages("caret")
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Convert columns to numeric ----------------------------------------------
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Split data for training and validating ----------------------------------
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
# Create trainControl to be reused by all models --------------------------
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(training.df[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#######################
# Generate each model #
##############
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
library(klaR)
library(caret)
#libraries
install.packages("caret")
install.packages("caret")
library(caret)
library(klaR)
install.packages("httpuv")
library(httpuv)
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
install.packages("pROC")
library(pROC) # for AUC calculations
library(devtools)
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
library(klaR)
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
install.packages("klaR")
library(klaR)
install.packages("httpuv")
install.packages("httpuv")
library(httpuv)
library(klaR)
library(caret)
# Naive Bayes -------------------------------------------------------------
nb<- train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
# Naive Bayes -------------------------------------------------------------
nb<-  caret::train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
