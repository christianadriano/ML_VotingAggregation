svmModel<-train(Selected~Ranking, myData,method="svmRadial", trControl=myControl);
resampleList<-resamples(list(rf=rfModel,glm=glmModel,knn=knnModel, svm=svmModel))
bwplot(resampleList,metric="ROC")
myFolds <- createFolds(myData$Ranking, k=7);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
knnModel<-train(Selected~Ranking, myData,method="knn", trControl=myControl);
rfModel<-train(Selected~Ranking, myData,method="rf", trControl=myControl);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
svmModel<-train(Selected~Ranking, myData,method="svmRadial", trControl=myControl);
resampleList<-resamples(list(rf=rfModel,glm=glmModel,knn=knnModel, svm=svmModel))
bwplot(resampleList,metric="ROC")
myFolds <- createFolds(myData$Ranking, k=7);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
knnModel<-train(Selected~Ranking, myData,method="knn", trControl=myControl);
rfModel<-train(Selected~Ranking, myData,method="rf", trControl=myControl);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
svmModel<-train(Selected~Ranking, myData,method="svmRadial", trControl=myControl);
resampleList<-resamples(list(rf=rfModel,glm=glmModel,knn=knnModel, svm=svmModel))
bwplot(resampleList,metric="ROC")
glmModel
densityplot(resampleList,metric="ROC")
dotplot(resampleList,xlim=range(0,1),metric="ROC")
xyplot(resampleList,xlim=range(0,1), metric="ROC")
myFolds <- createFolds(myData$Ranking, k=4);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=3);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
myFolds <- createFolds(myData$Ranking, k=3);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=5);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
myFolds <- createFolds(myData$Ranking, k=2);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=4);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=5);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=6);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
myFolds <- createFolds(myData$Ranking, k=7);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
glmModel
predict(glmModel,myData)
predict?
?predict
predict(myData)
predict(knnModel,myData)
myFolds <- createFolds(myData$Ranking, k=5);
set.seed(1234);
myControl<-trainControl(index=myFolds, classProbs=TRUE, summaryFunction=twoClassSummary);
knnModel<-train(Selected~Ranking, myData,method="knn", trControl=myControl);
rfModel<-train(Selected~Ranking, myData,method="rf", trControl=myControl);
glmModel<-train(Selected~Ranking, myData,method="glm", trControl=myControl);
svmModel<-train(Selected~Ranking, myData,method="svmRadial", trControl=myControl);
predict(glmModel,myData)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
library(class);
library(gmodels);
library(caret);
library(e1071)
library(ggplot2);
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];#reorder the rows based on a random index
#convert columns to numeric
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
#Select only the ranking as a feature to predict bugCovering
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
#build model
fitModel.cv <- knn.cv(trainingData, trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
fitModel.cv.df<-data.frame(fitModel.cv);
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList) #mean vote
min(predictedList) #highest ranking
max(predictedList) #lowest ranking
predictedList.df <- data.frame(predictedList);
colnames(predictedList.df)<- c("votes");
ggplot(data=predictedList.df, aes(x=predictedList.df$votes)) +
geom_histogram(binwidth = 0.5,alpha=.5, position="identity")+
geom_vline(aes(xintercept=mean(predictedList.df$votes, na.rm=T)),   # Ignore NA values for mean
color="red", linetype="dashed", size=1) +
ggtitle("Ranking of questions predicted as bug covering")+
labs(x="Ranking of YES votes of the questions categorized as bug-covering. lowest ranking=3, mean=1.71",
y="Frequency");
set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number=10, p=0.9, repeats = 5)
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
mean(trainingData$rankingVote);
knn_fit <- train(bugCovering ~ rankingVote, data = trainingData, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
bugCoveringPredicted <- predict(knn_fit,newdata = trainingData);
confusionMatrix(data=bugCoveringPredicted,trainingData$bugCovering)
df<-data.frame(bugCoveringPredicted)
predictedBugCoveringList<-trainingData[df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., knn caret repeatedcv, mean=1.52, max=2",xlab="ranking by number of YES's");
?confusionMatrix
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
library(class);
library(gmodels);
library(caret);
library(e1071)
library(ggplot2);
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];#reorder the rows based on a random index
#convert columns to numeric
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
#Select only the ranking as a feature to predict bugCovering
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
#build model
fitModel.cv <- knn.cv(trainingData, trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
fitModel.cv.df<-data.frame(fitModel.cv);
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList) #mean vote
min(predictedList) #highest ranking
max(predictedList) #lowest ranking
predictedList.df <- data.frame(predictedList);
colnames(predictedList.df)<- c("votes");
ggplot(data=predictedList.df, aes(x=predictedList.df$votes)) +
geom_histogram(binwidth = 0.5,alpha=.5, position="identity")+
geom_vline(aes(xintercept=mean(predictedList.df$votes, na.rm=T)),   # Ignore NA values for mean
color="red", linetype="dashed", size=1) +
ggtitle("Ranking of questions predicted as bug covering")+
labs(x="Ranking of YES votes of the questions categorized as bug-covering. lowest ranking=3, mean=1.71",
y="Frequency");
set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number=10, p=0.9, repeats = 5)
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
mean(trainingData$rankingVote);
knn_fit <- train(bugCovering ~ rankingVote, data = trainingData, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
bugCoveringPredicted <- predict(knn_fit,newdata = trainingData);
confusionMatrix(data=bugCoveringPredicted,trainingData$bugCovering, mode="prec_recall", positive="TRUE")
df<-data.frame(bugCoveringPredicted)
predictedBugCoveringList<-trainingData[df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., knn caret repeatedcv, mean=1.52, max=2",xlab="ranking by number of YES's");
#libraries
library(caret)
####################
#Import data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =3 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
svmLinearWeightsModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="svmLinearWeights", trControl=kFoldControl, metric="Sens");
svmLinearWeightsModel
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
svmLinearWeightsModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="svmLinearWeights", trControl=kFoldControl, metric="Sens");
svmLinearWeightsModel
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =4 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
svmLinearWeightsModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="svmLinearWeights", trControl=kFoldControl, metric="Sens");
svmLinearWeightsModel
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =4 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
svmLinearWeightsModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="svmLinearWeights", trControl=kFoldControl, metric="Sens");
svmLinearWeightsModel
bugCoveringPredicted <- predict(svmLinearWeightsModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =7 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
svmLinearWeightsModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="svmLinearWeights", trControl=kFoldControl, metric="Sens");
svmLinearWeightsModel
bugCoveringPredicted <- predict(svmLinearWeightsModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =7 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
# svmLinearWeightsModel
bugCoveringPredicted <- predict(knn,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =4 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
# svmLinearWeightsModel
bugCoveringPredicted <- predict(knn,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =90 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knnMModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
# svmLinearWeightsModel
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
knnMModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =30 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
# svmLinearWeightsModel
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =3 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
# svmLinearWeightsModel
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
## Table to store the outcomes from the training and model selection and prediction
outcome <- matrix(ncol = 11, nrow = 100);
colnames(outcome)<- c("kfolds","trainingError","accuracy","trueNegatives","truePositives",
"falseNegatives","falsePositives","precision","recall","specificity","sensitivity");
for(folds in 2:100){
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =folds );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
knnModel
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
matrixResult<- confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
trueNegatives<- matrixResult$table[1,1];
truePositives<- matrixResult$table[2,2];
falseNegatives<- matrixResult$table[1,2];
falsePositives<- matrixResult$table[2,1];
accuracy <- (truePositives + trueNegatives) / (truePositives + trueNegatives + falsePositives + falseNegatives);
trainingError <- 1-accuracy;
precision <- truePositives / (truePositives + falsePositives);
recall <- truePositives / (truePositives + falseNegatives);
sensitivity <- trueNegatives / (trueNegatives + falsePositives);
specificity <- truePositives / (truePositives + falseNegatives);
row <- folds-1;
outcome[row,"kfolds"]<-folds;
outcome[row,"trainingError"]<-trainingError;
outcome[row,"accuracy"]<-accuracy;
outcome[row,"trueNegatives"]<-trueNegatives;
outcome[row,"truePositives"]<-truePositives;
outcome[row,"falseNegatives"]<-falseNegatives;
outcome[row,"falsePositives"]<-falsePositives;
outcome[row,"precision"]<-precision;
outcome[row,"recall"]<-recall;
outcome[row,"sensitivity"]<-sensitivity;
outcome[row,"specificity"]<-specificity;
}
write.csv(outcome, file = "knn_kfold_study.csv");
write.csv(outcome, file = "/kfold-study/knn_kfold_study.csv");
write.csv(outcome, file = "kfold-stud/knn_kfold_study.csv");
write.csv(outcome, file = "./kfold-stud/knn_kfold_study.csv");
write.csv(outcome, file = ".//kfold-stud//knn_kfold_study.csv");
write.csv(outcome, file = "//kfold-stud//knn_kfold_study.csv");
write.csv(outcome, file = "C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//kfold-stud//knn_kfold_study.csv");
