# Naive Bayes -------------------------------------------------------------
nb<-  caret::train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
nb
#AM.1
#AM.2
#usekernel  ROC        Sens       Spec
#FALSE      0.7136796  0.9380167  0.4383509
#TRUE       0.7241444  0.9302171  0.4439717
#AM.3:
# usekernel  ROC        Sens       Spec
# FALSE      0.7546970  0.9031409  0.5664596
# TRUE       0.7534538  0.9270484  0.5095109
# KNN ---------------------------------------------------------------------
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
knn
#Aggre. k  ROC        Sens       Spec
#AM.1:
#AM.2: 7  0.8338240  0.9851064  0.0750000
#AM.3: 5  0.8290137  0.9778947  0.1340909
# Random Forest -----------------------------------------------------------
rf<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
rf
#Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.7938766  0.8638338  0.4812422
#AM.3: 0.8124545  0.8762876  0.5132246
# xgBoostTree -------------------------------------------------------------
xgbtree <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="gbm", trControl=kFoldControl);
xgbtree
# GLM ---------------------------------------------------------------------
glmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glm", trControl=kFoldControl)
glmModel
#Aggre. ROC        Sens       Spec
#AM.1:
#AM.2:  0.8276035  0.9338004  0.4748377
#AM.3:  0.8747113  0.9237826  0.4507378
# Bayes GLM ---------------------------------------------------------------
bayesglm<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
bayesglm
# Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.8797804  0.9338004  0.4748377
#AM.3: 0.8898239  0.9322932  0.4371014
#Not part of Caret and produced results similar to bayesglm
#glmBoost<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmBoost", trControl=kFoldControl);
#Not working
#glmnet<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmnet", trControl=kFoldControl);
#glmnet model is a more sophisticated solution that uses penalty terms to reduce the magnitude
#of the two GLM coeficients. The goal of GMLNet is to explain as much variance in the model.
#The trade-off is that glmnet accepts more bias in the data (more risk of overfitting)
#In any case, both glmnet and glm produce the exact same results for my data, therefore I favored
#the simplest model.
# SVM ---------------------------------------------------------------------
svmLinear <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear", trControl=kFoldControl);
svmLinear2 <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear2", trControl=kFoldControl);
svmLinearWeights <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights", trControl=kFoldControl, metric="Spec");
svmLinear
#Aggre.    ROC        Sens       Spec
#AM.1:
#AM.2: 0.598301  0.9589485  0.2627181
#AM.3: 0.6798618  0.9643897  0.2357955
svmLinear2
#Aggre. cost  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  0.8009603  0.9616318  0.2776515
#AM.3: 0.50  0.7713566  0.9757671  0.1613636
svmLinearWeights
#Aggre. cost  weight  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  2       0.8016421  0.9082113  0.6105458
#AM.3: 0.50  3       0.8102679  0.8439114  0.5645059
###################
#Visualize models
resampleList<-resamples(list(svmLinear=svmLinear,svmLinear2=svmLinear2,svmLinearWeights=svmLinearWeights,
glm=glmModel,bayesglm=bayesglm, rf=rf, knn=knn, nb=nb
));
bwplot(resampleList,metric="ROC")
densityplot(resampleList,metric="ROC")
dotplot(resampleList,xlim=range(0,1),metric="ROC")
##################################################
#Predict n based on best model
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(nb,validation.df),
predict(knn,validation.df),
predict(rf,validation.df),
predict(bayesglm,validation.df),
predict(svmLinearWeights,validation.df)
);
colnames(compareTable) <- c("explanatoryVariable","actual","nb","knn","rf","glm","svm");
####################################################
compareTable
View(compareTable)
predictedBugCoveringList<-compareTable[compareTable$predicted=="T",];
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(bayesglm,validation.df)
);
colnames(compareTable) <- c("explanatoryVariable","actual","predicted");
####################################################
compareTable
predictedBugCoveringList<-compareTable[compareTable$predicted=="T",];
predictedBugCoveringList$explanatoryVariable
predictedBugCoveringList
#Computing the miminum value of n that predicted bugCovering True
min(predictedBugCoveringList$explanatoryVariable);
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Convert columns to numeric ----------------------------------------------
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"])); #AM.3
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"])); #AM.2
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"])); #AM.1
summaryTable[,"explanatoryVariable"] <- summaryTable[,"rankingVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Split data for training and validating ----------------------------------
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 0.95);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
# Create trainControl to be reused by all models --------------------------
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(training.df[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#######################
# Generate each model #
##############
# Naive Bayes -------------------------------------------------------------
nb<-  caret::train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
nb
#AM.1
#AM.2
#usekernel  ROC        Sens       Spec
#FALSE      0.7136796  0.9380167  0.4383509
#TRUE       0.7241444  0.9302171  0.4439717
#AM.3:
# usekernel  ROC        Sens       Spec
# FALSE      0.7546970  0.9031409  0.5664596
# TRUE       0.7534538  0.9270484  0.5095109
# KNN ---------------------------------------------------------------------
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
knn
#Aggre. k  ROC        Sens       Spec
#AM.1:
#AM.2: 7  0.8338240  0.9851064  0.0750000
#AM.3: 5  0.8290137  0.9778947  0.1340909
# Random Forest -----------------------------------------------------------
rf<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
rf
#Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.7938766  0.8638338  0.4812422
#AM.3: 0.8124545  0.8762876  0.5132246
# xgBoostTree -------------------------------------------------------------
xgbtree <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="gbm", trControl=kFoldControl);
xgbtree
# GLM ---------------------------------------------------------------------
glmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glm", trControl=kFoldControl)
glmModel
#Aggre. ROC        Sens       Spec
#AM.1:
#AM.2:  0.8276035  0.9338004  0.4748377
#AM.3:  0.8747113  0.9237826  0.4507378
# Bayes GLM ---------------------------------------------------------------
bayesglm<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
bayesglm
# Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.8797804  0.9338004  0.4748377
#AM.3: 0.8898239  0.9322932  0.4371014
#Not part of Caret and produced results similar to bayesglm
#glmBoost<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmBoost", trControl=kFoldControl);
#Not working
#glmnet<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmnet", trControl=kFoldControl);
#glmnet model is a more sophisticated solution that uses penalty terms to reduce the magnitude
#of the two GLM coeficients. The goal of GMLNet is to explain as much variance in the model.
#The trade-off is that glmnet accepts more bias in the data (more risk of overfitting)
#In any case, both glmnet and glm produce the exact same results for my data, therefore I favored
#the simplest model.
# SVM ---------------------------------------------------------------------
svmLinear <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear", trControl=kFoldControl);
svmLinear2 <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear2", trControl=kFoldControl);
svmLinearWeights <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights", trControl=kFoldControl, metric="Spec");
svmLinear
#Aggre.    ROC        Sens       Spec
#AM.1:
#AM.2: 0.598301  0.9589485  0.2627181
#AM.3: 0.6798618  0.9643897  0.2357955
svmLinear2
#Aggre. cost  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  0.8009603  0.9616318  0.2776515
#AM.3: 0.50  0.7713566  0.9757671  0.1613636
svmLinearWeights
#Aggre. cost  weight  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  2       0.8016421  0.9082113  0.6105458
#AM.3: 0.50  3       0.8102679  0.8439114  0.5645059
##################################################
#Predict n based on best model
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(nb,validation.df),
predict(knn,validation.df),
predict(rf,validation.df),
predict(bayesglm,validation.df),
predict(svmLinearWeights,validation.df)
);
colnames(compareTable) <- c("explanatoryVariable","actual","nb","knn","rf","glm","svm");
View(compareTable)
####################################################
#Predict n based on best model
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(bayesglm,validation.df)
);
colnames(compareTable) <- c("explanatoryVariable","actual","predicted");
View(compareTable)
predictedBugCoveringList
predictedBugCoveringList<-compareTable[compareTable$predicted=="T",];
predictedBugCoveringList$explanatoryVariable
predictedBugCoveringList
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Convert columns to numeric ----------------------------------------------
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"])); #AM.3
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"])); #AM.2
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"])); #AM.1
summaryTable[,"explanatoryVariable"] <- summaryTable[,"rankingVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Split data for training and validating ----------------------------------
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 1);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
# Create trainControl to be reused by all models --------------------------
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(training.df[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#######################
# Generate each model #
##############
# Naive Bayes -------------------------------------------------------------
nb<-  caret::train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
nb
#AM.1
#AM.2
#usekernel  ROC        Sens       Spec
#FALSE      0.7136796  0.9380167  0.4383509
#TRUE       0.7241444  0.9302171  0.4439717
#AM.3:
# usekernel  ROC        Sens       Spec
# FALSE      0.7546970  0.9031409  0.5664596
# TRUE       0.7534538  0.9270484  0.5095109
# KNN ---------------------------------------------------------------------
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
knn
#Aggre. k  ROC        Sens       Spec
#AM.1:
#AM.2: 7  0.8338240  0.9851064  0.0750000
#AM.3: 5  0.8290137  0.9778947  0.1340909
# Random Forest -----------------------------------------------------------
rf<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
rf
#Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.7938766  0.8638338  0.4812422
#AM.3: 0.8124545  0.8762876  0.5132246
# xgBoostTree -------------------------------------------------------------
xgbtree <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="gbm", trControl=kFoldControl);
xgbtree
# GLM ---------------------------------------------------------------------
glmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glm", trControl=kFoldControl)
glmModel
#Aggre. ROC        Sens       Spec
#AM.1:
#AM.2:  0.8276035  0.9338004  0.4748377
#AM.3:  0.8747113  0.9237826  0.4507378
# Bayes GLM ---------------------------------------------------------------
bayesglm<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
bayesglm
# Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.8797804  0.9338004  0.4748377
#AM.3: 0.8898239  0.9322932  0.4371014
#Not part of Caret and produced results similar to bayesglm
#glmBoost<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmBoost", trControl=kFoldControl);
#Not working
#glmnet<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmnet", trControl=kFoldControl);
#glmnet model is a more sophisticated solution that uses penalty terms to reduce the magnitude
#of the two GLM coeficients. The goal of GMLNet is to explain as much variance in the model.
#The trade-off is that glmnet accepts more bias in the data (more risk of overfitting)
#In any case, both glmnet and glm produce the exact same results for my data, therefore I favored
#the simplest model.
# SVM ---------------------------------------------------------------------
svmLinear <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear", trControl=kFoldControl);
svmLinear2 <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear2", trControl=kFoldControl);
svmLinearWeights <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights", trControl=kFoldControl, metric="Spec");
svmLinear
#Aggre.    ROC        Sens       Spec
#AM.1:
#AM.2: 0.598301  0.9589485  0.2627181
#AM.3: 0.6798618  0.9643897  0.2357955
svmLinear2
#Aggre. cost  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  0.8009603  0.9616318  0.2776515
#AM.3: 0.50  0.7713566  0.9757671  0.1613636
svmLinearWeights
#Aggre. cost  weight  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  2       0.8016421  0.9082113  0.6105458
#AM.3: 0.50  3       0.8102679  0.8439114  0.5645059
####################################################
#Predict n based on best model
compareTable <- data.frame(summaryTable$explanatoryVariable,
summaryTable$bugCoveringLabels,
predict(bayesglm,summaryTable)
);
colnames(compareTable) <- c("explanatoryVariable","actual","predicted");
####################################################
compareTable
predictedBugCoveringList<-compareTable[compareTable$predicted=="T",];
predictedBugCoveringList$explanatoryVariable
source("C://Users//Chris//Documents//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
# Convert columns to numeric ----------------------------------------------
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"])); #AM.3
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"])); #AM.2
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"])); #AM.1
summaryTable[,"explanatoryVariable"] <- summaryTable[,"rankingVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Split data for training and validating ----------------------------------
totalData.size <- dim(summaryTable)[1];
training.size <- trunc(totalData.size * 0.7);
training.df <- as.data.frame(summaryTable[1:training.size-1,]);
validation.df <- as.data.frame(summaryTable[training.size:totalData.size,]);
# Create trainControl to be reused by all models --------------------------
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(training.df[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#######################
# Generate each model #
##############
# Naive Bayes -------------------------------------------------------------
nb<-  caret::train(bugCoveringLabels ~ explanatoryVariable,training.df, method="nb", trControl=kFoldControl);
nb
#AM.1
#AM.2
#usekernel  ROC        Sens       Spec
#FALSE      0.7136796  0.9380167  0.4383509
#TRUE       0.7241444  0.9302171  0.4439717
#AM.3:
# usekernel  ROC        Sens       Spec
# FALSE      0.7546970  0.9031409  0.5664596
# TRUE       0.7534538  0.9270484  0.5095109
# KNN ---------------------------------------------------------------------
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
knn
#Aggre. k  ROC        Sens       Spec
#AM.1:
#AM.2: 7  0.8338240  0.9851064  0.0750000
#AM.3: 5  0.8290137  0.9778947  0.1340909
# Random Forest -----------------------------------------------------------
rf<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
rf
#Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.7938766  0.8638338  0.4812422
#AM.3: 0.8124545  0.8762876  0.5132246
# xgBoostTree -------------------------------------------------------------
xgbtree <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="gbm", trControl=kFoldControl);
xgbtree
# GLM ---------------------------------------------------------------------
glmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glm", trControl=kFoldControl)
glmModel
#Aggre. ROC        Sens       Spec
#AM.1:
#AM.2:  0.8276035  0.9338004  0.4748377
#AM.3:  0.8747113  0.9237826  0.4507378
# Bayes GLM ---------------------------------------------------------------
bayesglm<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
bayesglm
# Aggre.  ROC        Sens       Spec
#AM.1:
#AM.2: 0.8797804  0.9338004  0.4748377
#AM.3: 0.8898239  0.9322932  0.4371014
#Not part of Caret and produced results similar to bayesglm
#glmBoost<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmBoost", trControl=kFoldControl);
#Not working
#glmnet<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="glmnet", trControl=kFoldControl);
#glmnet model is a more sophisticated solution that uses penalty terms to reduce the magnitude
#of the two GLM coeficients. The goal of GMLNet is to explain as much variance in the model.
#The trade-off is that glmnet accepts more bias in the data (more risk of overfitting)
#In any case, both glmnet and glm produce the exact same results for my data, therefore I favored
#the simplest model.
# SVM ---------------------------------------------------------------------
svmLinear <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear", trControl=kFoldControl);
svmLinear2 <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinear2", trControl=kFoldControl);
svmLinearWeights <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights", trControl=kFoldControl, metric="Spec");
svmLinear
#Aggre.    ROC        Sens       Spec
#AM.1:
#AM.2: 0.598301  0.9589485  0.2627181
#AM.3: 0.6798618  0.9643897  0.2357955
svmLinear2
#Aggre. cost  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  0.8009603  0.9616318  0.2776515
#AM.3: 0.50  0.7713566  0.9757671  0.1613636
svmLinearWeights
#Aggre. cost  weight  ROC        Sens       Spec
#AM.1:
#AM.2: 1.00  2       0.8016421  0.9082113  0.6105458
#AM.3: 0.50  3       0.8102679  0.8439114  0.5645059
# Compare models ----------------------------------------------------------
####################################################
#Predict n based on best model
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(bayesglm,summaryTable)
);
####################################################
#Predict n based on best model
compareTable <- data.frame(validation.df$explanatoryVariable,
validation.df$bugCoveringLabels,
predict(bayesglm,validation.df)
);
colnames(compareTable) <- c("explanatoryVariable","actual","predicted");
####################################################
compareTable
predictedBugCoveringList<-compareTable[compareTable$predicted=="T",];
predictedBugCoveringList$explanatoryVariable
#Computing the miminum value of n that predicted bugCovering True
min(predictedBugCoveringList$explanatoryVariable);
#Computing the miminum value of n that predicted bugCovering True
max(predictedBugCoveringList$explanatoryVariable);
max(predictedBugCoveringList$explanatoryVariable);
#Computing the miminum value of n that predicted bugCovering True
min(predictedBugCoveringList$explanatoryVariable);
