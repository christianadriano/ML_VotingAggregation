fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=1, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=7, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=1, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=1, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=2, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=4, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=5, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=7, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=9, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=4, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","YES.Count")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$YES.Count);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=4, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
trainingData <- summaryTable[,c("bugCovering","YES.Count")];
trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData$rankingVote <- as.numeric(trainingData$YES.Count);
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData$rankingVote <- as.numeric(trainingData$YES.Count);
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$YES.Count);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=5, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=7, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$YES.Count);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=9, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
View(summaryTable)
View(summaryTable)
trainingData <- summaryTable[,c("bugCovering","MajorityVote")];
trainingData <- summaryTable[,c("bugCovering","Majority.Vote")];
View(summaryTable)
View(summaryTable)
trainingData <- summaryTable[,c("bugCovering","majorityVote")];
trainingData$rankingVote <- as.numeric(trainingData$majorityVote);
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=9, l=0, prob = FALSE, use.all=TRUE);
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","majorityVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$majorityVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=7, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=5, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","majorityVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$majorityVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
#Scramble the dataset before extracting the training set.
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","majorityVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$majorityVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
#head(summaryTable)
##################################################################
#Build the KNN model
#Select only the ranking as a feature to predict bugCovering
#trainingData <- summaryTable[,c("bugCovering","Yes.Count")];
trainingData <- summaryTable[,c("bugCovering","majorityVote")];
#Prepare explanatory variable (rankingVote) and target (bugCovering)
#trainingData <-data.frame(summaryTable);
trainingData$rankingVote <- as.numeric(trainingData$majorityVote);
######################################################################################
#Using KNN from CLASS package
fitModel.cv <- knn.cv (train =trainingData, cl=trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
#Evaluate model
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
plot(fitModel.cv)
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(predictedList)
min(predictedList)
max(predictedList)
