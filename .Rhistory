colnames(predictedList.df)<- c("votes");
ggplot(data=predictedList.df, aes(x=predictedList.df$votes)) +
geom_histogram(binwidth = 0.5,alpha=.5, position="identity")+
geom_vline(aes(xintercept=mean(predictedList.df$votes, na.rm=T)),   # Ignore NA values for mean
color="red", linetype="dashed", size=1) +
ggtitle("Ranking of questions predicted as bug covering")+
labs(x="Ranking of YES votes of the questions categorized as bug-covering. lowest ranking=3, mean=1.71",
y="Frequency");
set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number=10, p=0.9, repeats = 5)
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
mean(trainingData$rankingVote);
knn_fit <- train(bugCovering ~ rankingVote, data = trainingData, method = "knn",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
bugCoveringPredicted <- predict(knn_fit,newdata = trainingData);
confusionMatrix(data=bugCoveringPredicted,trainingData$bugCovering, mode="prec_recall", positive="TRUE")
df<-data.frame(bugCoveringPredicted)
predictedBugCoveringList<-trainingData[df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., knn caret repeatedcv, mean=1.52, max=2",xlab="ranking by number of YES's");
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k = 10);
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
library(caret)
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#######################
# Generate each model #
knn <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
knn$results
knn$pred
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k =3 );
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with k folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
#knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="knn", trControl=kFoldControl);
#rfModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="rf", trControl=kFoldControl);
#bayesglmModel<- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="bayesglm", trControl=kFoldControl);
knnModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
#check if n changes if I optimize for False Negatives (Sensitivity), but need to know what is the Positive class for training
knnModel
bugCoveringPredicted <- predict(knnModel,newdata = summaryTable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
?roc
library(pROC) # for AUC calculations
install.packages("pROC")
library(pROC) # for AUC calculations
?roc
?auc
pROC::auc()
?pROC::auc()
?pROC::auc
roc(fitModel,summaryTable);
fitModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Sens");
roc(fitModel,summaryTable);
roc(summaryTable$bugCoveringLabels,bugCoveringPredicted);
bugCoveringPredicted <- predict(fitModel,newdata = summaryTable);
matrixResult<- confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
roc(summaryTable$bugCoveringLabels,bugCoveringPredicted);
roc(as.numeric(summaryTable$bugCoveringLabels),as.numeric(bugCoveringPredicted));
roc(response = as.numeric(summaryTable$bugCoveringLabels),predictor = as.numeric(bugCoveringPredicted)) %>% auc()
roc(response = as.numeric(summaryTable$bugCoveringLabels),predictor = as.numeric(bugCoveringPredicted)) %>% auc()%>%aucValue;
aucValue<- roc(response = as.numeric(summaryTable$bugCoveringLabels),predictor = as.numeric(bugCoveringPredicted)) %>% auc();
aucValue
aucValue
rocValue<-roc(response = as.numeric(summaryTable$bugCoveringLabels),predictor = as.numeric(bugCoveringPredicted));
rocValue$auc
as.numeric(rocValue$auc)
?prSummary
varImp(fitModel)
varImp(fitModel,scale=FALSE)
plot(fitModel)
fitModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Spec");
fitModel
plot(fitMode)
plot(fitModel)
fitModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="ROC");
plot(fitModel)
fitModel <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable,
method="knn", trControl=kFoldControl, metric="Spec");
fitModel
bugCoveringPredicted <- predict(fitModel,newdata = summaryTable);
matrixResult<- confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
bugCoveringPredicted
predictedBugCoveringList<-summaryTable[fitModel[,1]==T,];
fitModel[,1]
fitModel$pred
bugCoveringPredicted[,1]
library(caret)
####################
#Import data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
compareTable <- data.frame(summaryTable$explanatoryVariable,
summaryTable$bugCoveringLabels,
predict(svmLinearWeights,summaryTable)
);
colnames(compareTable) <- c("explanatoryVariable","actual","svm");
svmLinearWeights <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights", trControl=kFoldControl, metric="Spec");
compareTable <- data.frame(summaryTable$explanatoryVariable,
summaryTable$bugCoveringLabels,
predict(svmLinearWeights,summaryTable)
);
colnames(compareTable) <- c("explanatoryVariable","actual","svm");
compareTable
predictedBugCoveringList<-compareTable[compareTable$svm=="T",];
predictedBugCoveringList$explanatoryVariable
predictedBugCoveringList
min(predictedBugCoveringList$explanatoryVariable);
library(caret)
####################
#Import data
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
svmLinearWeights_Spec <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Spec");
svmLinearWeights_Sens <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Sens");
svmLinearWeights_Kappa <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Kappa");
svmLinearWeights_Accuracy <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Accuracy");
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="MAE");
svmLinearWeights_ROC <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="ROC");
svmLinearWeights_Spec <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Spec");
svmLinearWeights_Sens <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Sens");
svmLinearWeights_Kappa <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Kappa");
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="MAE");
resampleList<-resamples(list(svm_Spec=svmLinearWeights_Spec,
svm_Sens=svmLinearWeights_Sens,
svm_Kappa=svmLinearWeights_Kappa,
svm_Accuracy=svmLinearWeights_Accuracy,
svm_MAE=svmLinearWeights_MAE,
svm_ROC=svmLinearWeights_ROC
));
bwplot(resampleList,metric="ROC")
densityplot(resampleList,metric="ROC")
dotplot(resampleList,xlim=range(0,1),metric="ROC")
bwplot(resampleList,metric="c")
bwplot(resampleList,metric="c")
bwplot(resampleList,metric="Spec")
bwplot(resampleList,metric="Sens")
bwplot(resampleList,metric="Accu")
bwplot(resampleList,metric="Accur")
bwplot(resampleList,metric="Accuracy")
bwplot(resampleList,metric="Acc")
bwplot(resampleList,metric="Kappa")
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="MAE");
library(mlbench)
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="MAE");
svmLinearWeights_Kappa <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Kappa");
svmLinearWeights_Spec
maeSummary <- function (data,
lev = NULL,
model = NULL) {
out <- mae(data$obs, data$pred)
names(out) <- "MAE"
out
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = maeSummary
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
library(mlbench)
?MAE
maeSummary <- function (data,
lev = NULL,
model = NULL) {
out <- MAE(data$obs, data$pred)
names(out) <- "MAE"
out
}
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = maeSummary
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
library(Metrics)
library(metrics)
maeFunction <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- sum(abs(labels - preds))/length(labels)
return(list(metric = "mae", err))
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = maeFunction
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
maeFunction <- function(preds, dtrain) {
labels <- getinfo(dtrain, "label")
err <- sum(abs(labels - preds))/length(labels)
return(list(metric = "MAE", err))
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = maeFunction
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
install.packages("Metrics")
library(Metrics)
maeSummary <- function (data,
lev = NULL,
model = NULL) {
out <- MAE(data$obs, data$pred)
names(out) <- "MAE"
out
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = maeSummary
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
mae_metric <- function (data,
lev = NULL,
model = NULL) {
out <- mae(exp(data$obs), exp(data$pred))
names(out) <- "MAE"
out
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = mae_metric
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
# Custom MAE metric in caret format
mae_metric <- function (data,
lev = NULL,
model = NULL) {
out <- mae(as.numeric(data$obs),as.numeric(data$pred))
names(out) <- "MAE"
out
}
# Create reusable trainControl object: myControl
kFoldControl_MAE <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = mae_metric
);
svmLinearWeights_MAE <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl_MAE, metric="MAE");
svmLinearWeights_MAE
svmLinearWeights_Kappa <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Kappa");
svmLinearWeights_Accuracy <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Accuracy");
compareTable <- data.frame(summaryTable$explanatoryVariable,
summaryTable$bugCoveringLabels,
predict(svmLinearWeights_MAE,summaryTable)
);
colnames(compareTable) <- c("explanatoryVariable","actual","svm");
predictedBugCoveringList<-compareTable[compareTable$svm=="T",];
predictedBugCoveringList$explanatoryVariable
predictedBugCoveringList
#Computing the miminum value of n that predicted bugCovering True
min(predictedBugCoveringList$explanatoryVariable);
bugCoveringPredicted <- predict(svmLinearWeights_MAE,summaryTable);
compareTable <- data.frame(summaryTable$explanatoryVariable,
summaryTable$bugCoveringLabels,
predict(svmLinearWeights_MAE,summaryTable)
);
colnames(compareTable) <- c("explanatoryVariable","actual","svm");
predictedBugCoveringList<-compareTable[compareTable$svm=="T",];
predictedBugCoveringList$explanatoryVariable
predictedBugCoveringList
#Computing the miminum value of n that predicted bugCovering True
min(predictedBugCoveringList$explanatoryVariable);
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
?kappa
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="F");
confusionMatrix(data=bugCoveringPredicted,summaryTable$bugCoveringLabels, positive="T");
?MAE
?metric
?train
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();
#summaryTable <- data.frame(summaryTable);
#I need to guarantee that some examples (i.e., failing methods)
#do not dominate the training or testing sets. To do that, I need to get a
#close to equal proportion of examples in both sets
#Scramble the dataset before extracting the training set.
set.seed(8850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];
##################################################
# Create trainControl to be reused by all models #
#convert columns to numeric
summaryTable<- data.frame(summaryTable, stringsAsFactors = FALSE)
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"]));
summaryTable[,"Yes.Count"] <- as.numeric(unlist(summaryTable[,"Yes.Count"]));
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"]));
summaryTable[,"explanatoryVariable"] <- summaryTable[,"majorityVote"];
summaryTable$bugCoveringLabels <- as.character(summaryTable$bugCovering);
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="FALSE", "F");
summaryTable$bugCoveringLabels<- replace(summaryTable$bugCoveringLabels,summaryTable$bugCoveringLabels=="TRUE", "T");
summaryTable$bugCoveringLabels<- as.factor(summaryTable$bugCoveringLabels);
# Create custom indices: myFolds
#Guarantees that we are going to use the exact same datasets for all models
myFolds <- createFolds(summaryTable[,"explanatoryVariable"] , k = 10);
#larger K implies less bias (overfitting). However, larger K implies larger variance, i.e.,
#the prediction has large variation. The reason is that larger K makes each training data large and
#very similar.
#nice explanation here: https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation
# Create reusable trainControl object: myControl
kFoldControl <- trainControl(
index = myFolds, #Train with 9 folds and validate with one
classProbs = TRUE, # IMPORTANT!
verboseIter = TRUE, #
savePredictions = TRUE, #
summaryFunction = twoClassSummary
);
svmLinearWeights_Accuracy <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Accuracy");
svmLinearWeights_Accuracy <- train(bugCoveringLabels ~ explanatoryVariable,summaryTable, method="svmLinearWeights",
trControl=kFoldControl, metric="Accu");
