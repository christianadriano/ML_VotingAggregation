---
title: "K-NearestNeighbor - Predicting Bug Covering by Ranking"
author: "Christian Medeiros Adriano"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();

library(class);
library(gmodels);
library(caret);
library(e1071)
```

##The goal of the study
<p>
Making sense of a multiplce answers to one same question is challenging if one needs to extract some kind of consensus. The challenges relate to the different levels of skill, but also personal preferences. </p>

<p>In the case of my experiment, I asked workers if they believed that a code fragment was related to a unit test failure. Each question was asked to 20 different workers. Workers answered YES, NO, or I don´t know. </p>

<p>After receiving all the answers for each code fragments, I was left with the challenge of using these answers to predict the code fragments that are most probability related to the failure. I labeled as "bug covering" the questions that were about code fragments that turned out to be related to the unit test failure.</p>

<du>Whe study has two goals:
<li>
Train a machine learning algorithm that predicts whether a code fragment is related to a failure or not. For that, I originally devised different metrics. The metric that will explore in the following study consists of ranking the questions by the number of YES answers received. Questions that received the most number of YES answers are assigned ranking level 1.
</li>

<li> Compared CLASS and CARET implementations of k-nearest neighbor algorithm. Results showed that CLASS implementation produced fewer false positives than CARET. Note that this might result of overfitting of my use of CLASS configuration of knn.cv.
</li>

</du>

##Data preparation
<p>I need to guarantee that some examples (i.e., failing methods)
do not dominate the training or testing sets. To do that, I need to get a 
close to equal proportion of examples in both sets. I do that by 
scrambling the data.</p>

```{r dataprep}

set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];#reorder the rows based on a random index

#convert columns to numeric
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"])); 
summaryTable[,"majorityVote"] <- as.numeric(unlist(summaryTable[,"majorityVote"])); 
summaryTable[,"thresholdVote"] <- as.numeric(unlist(summaryTable[,"thresholdVote"])); 

#Select only the ranking as a feature to predict bugCovering
trainingData <- summaryTable[,c("bugCovering","rankingVote")];

```

##KNN from CLASS package
<p> I chose knn.cv (cross validation) so I can minimize the risk of lucky selection of training and testing set. 
<p> Cross validations is performed by leaving one out, however even when I set partition to 70/30 I obtained the same reaults.
<p> I also run with differnt levels of  k=3,5,7,9, which produced the same results as well
```{r knn.cv.class}
#build model
fitModel.cv <- knn.cv(trainingData, trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);
fitModel.cv.df<-data.frame(fitModel.cv)
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
 
```
<p>Compute the minimal ranking value that corresponded to the predicted bugCovering questions></p>
```{r knn.cv.ranking}
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., k=3, 5 or 7, knn Class, mean=1.71 ",xlab="ranking by number of YES");

```
<p></p>

## Why not use k=1?
<p>Last but not least, I also tried k=1, but the results seemed overfit the training set. The reason is that for k=1 the algorithm estimates the probability based on a single data point, which is the closest neighbor. This ends up being very sensitive to distortions such as outliers, noise, or mislabelling of data. Moreover, since I am using cross validation which uses the same dataset to train and test by leaving one ou, there is even higher risks of comparing the test data point against itself because the sampling is done with replacement. Therefore, I decided to use values larger than 3 and odd in order to avoid ties.</p>

<p>For a more detailed discussion on k=1 value, refer to https://stats.stackexchange.com/questions/107870/does-k-nn-with-k-1-always-implies-overfitting#107913 </p>



## KNN from CARET package
<p> https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf
<p> https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/

<p> I will do 5 repeats of 10-Fold CV. I will fit a KNN model that evaluates 10 values of k
```{r knn.caret.model, echo=FALSE}

set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number=10, p=0.9, repeats = 5)

trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
mean(trainingData$rankingVote);

knn_fit <- train(bugCovering ~ rankingVote, data = trainingData, method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10)

bugCoveringPredicted <- predict(knn_fit,newdata = trainingData);

confusionMatrix(data=bugCoveringPredicted,trainingData$bugCovering)
 

```
<p>Compute the minimal ranking value that corresponded to the predicted bugCovering questions
```{r knn.caret.ranking, echo=FALSE}
df<-data.frame(bugCoveringPredicted)
predictedBugCoveringList<-trainingData[df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., knn caret repeatedcv, mean=1.52, max=2",xlab="ranking by number of YES's");
```
<p></p>

## Conclusions
<p>Caret produced more false positives than knn.cv from class package. I tried to fine tune it more, but was not enough. The k value for Caret seems very high 15 t 23. However, these differences might be due to my use of knn.cv to be overfitting.</p>

<br><br>
