---
title: "K-Nearest Neighbor - Predicting Bug Covering by Ranking"
author: "Christian Medeiros Adriano"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
source("C://Users//chris//OneDrive//Documentos//GitHub//ML_VotingAggregation//aggregateAnswerOptionsPerQuestion.R");
summaryTable <- runMain();

library(class);
library(gmodels);
library(caret);
library(e1071)
library(ggplot2);
```

##The goal of the study
<p>
Making sense of multiple answers to one same question is challenging if one needs to extract some kind of consensus. The challenges relate to the different levels of skill, but also personal preferences. </p>

<p>In the case of my experiment, I asked workers if they believed that a code fragment was related to a unit test failure. Each question was asked to 20 different workers. Workers answered YES, NO, or I don´t know. </p>

<p>After receiving all the answers for each code fragments, I was left with the challenge of using these answers to predict the code fragments that are most probability related to the failure. I labeled as "bug covering" the questions that were about code fragments that turned out to be related to the unit test failure.</p>

<du>Whe study has two goals:
<li>
Train a machine learning algorithm that predicts whether a code fragment is related to a failure or not. For that, I originally devised different metrics. The metric that will explore in the following study consists of ranking the questions by the number of YES answers received. Questions that received the most number of YES answers are assigned ranking level 1.
</li>

<li> Compared CLASS and CARET implementations of k-nearest neighbor algorithm. Results showed that CLASS implementation produced fewer false positives than CARET. Note that this might result of overfitting of my use of CLASS configuration of knn.cv.
</li>

</du>

##Data preparation
<p>I need to guarantee that some examples (i.e., failing methods)
do not dominate the training or testing sets. To do that, I need to get a 
close to equal proportion of examples in both sets. I do that by 
scrambling the data.</p>

```{r dataprep}

set.seed(9850);
g<- runif((nrow(summaryTable))); #generates a random distribution
summaryTable <- summaryTable[order(g),];#reorder the rows based on a random index

#convert columns to numeric
summaryTable[,"rankingVote"] <- as.numeric(unlist(summaryTable[,"rankingVote"])); 

#Select only the ranking as a feature to predict bugCovering
trainingData <- summaryTable[,c("bugCovering","rankingVote")];
trainingData$rankingVote <- as.numeric(trainingData$rankingVote);

```

##KNN from CLASS package
<p> I chose knn.cv (cross validation) so I can minimize the risk of lucky selection of training and testing set. 
<p> Cross validations is performed by leaving one out, however even when I set partition to 70/30 I obtained the same reaults.
<p> I also run with differnt levels of  k=3,5,7,9, which produced the same results as well
```{r knn.cv.class}
#build model
fitModel.cv <- knn.cv(trainingData, trainingData$bugCovering, k=3, l=0, prob = FALSE, use.all=TRUE);

```

##Testing the model
```{r test.knn.cv.class, include=TRUE}
fitModel.cv.df<-data.frame(fitModel.cv);
CrossTable(x = trainingData$bugCovering, y=fitModel.cv.df[,1], prop.chisq = FALSE)
```

##Estimating the metric
<p>Discover the minimal ranking value that would have predicted the same bug Covering questions</p>
```{r knn.cv.metric, echo=FALSE}
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
predictedBugCoveringList<-trainingData[fitModel.cv.df[,1]==TRUE,];
predictedList <- as.numeric(unlist(predictedBugCoveringList[,2]));
```

<p>Mean ranking vote of the questions categorized as bug covering:</p>
```{r knn.cv.mean, echo=FALSE}
mean(predictedList) #mean vote
```

<p>Top ranking vote of the questions categorized as bug covering:</p>
```{r knn.cv.highest, echo=FALSE}
min(predictedList) #highest ranking 
```

<p>Lowest ranking vote of the questions still categorized as bug covering:</p>
```{r knn.cv.lowest, echo=FALSE}
max(predictedList) #lowest ranking
```

       
##Plot metric distribution
```{r plot, include=TRUE}
predictedList.df <- data.frame(predictedList);
colnames(predictedList.df)<- c("votes");

ggplot(data=predictedList.df, aes(x=predictedList.df$votes)) +
  geom_histogram(binwidth = 0.5,alpha=.5, position="identity")+
  geom_vline(aes(xintercept=mean(predictedList.df$votes, na.rm=T)),   # Ignore NA values for mean
             color="red", linetype="dashed", size=1) +
  ggtitle("Ranking of questions predicted as bug covering")+
  labs(x="Ranking of YES votes of the questions categorized as bug-covering. lowest ranking=3, mean=1.71", 
       y="Frequency");

```
<p></p>

## Why not use k=1?
<p>Last but not least, I also tried k=1, but the results seemed to overfit the training set. The reason is that for k=1 the algorithm estimates the probability based on a single data point, which is the closest neighbor. This ends up being very sensitive to distortions such as outliers, noise, or data mislabelling. Moreover, since I am using cross-validation which uses the same dataset to train and test by leaving one out, there is a even higher risks of comparing the test data point against itself because the sampling is done with replacement. Therefore, I decided to use values of k larger than 3 and odd in order to avoid ties.</p>

<p>For a more detailed discussion on k=1 value, refer to https://stats.stackexchange.com/questions/107870/does-k-nn-with-k-1-always-implies-overfitting#107913 </p>



## KNN from CARET package
<p> https://cran.r-project.org/web/packages/caret/vignettes/caret.pdf
<p> https://dataaspirant.com/2017/01/09/knn-implementation-r-using-caret-package/

<p> I will do 5 repeats of 10-Fold CV. I will fit a KNN model that evaluates 10 values of k
```{r knn.caret.model, echo=FALSE}

set.seed(1234)
trctrl <- trainControl(method = "repeatedcv", number=10, p=0.9, repeats = 5)

trainingData$rankingVote <- as.numeric(trainingData$rankingVote);
trainingData$bugCovering <- as.factor(trainingData$bugCovering);
mean(trainingData$rankingVote);

knn_fit <- train(bugCovering ~ rankingVote, data = trainingData, method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10);
```
### Model selection
```{r knn.caret.model.fit, echo=TRUE}
knn_fit
```
#### Confusion matrix using the selected model
```{r knn.caret.model.confusion, echo=TRUE}

bugCoveringPredicted <- predict(knn_fit,newdata = trainingData);

confusionMatrix(data=bugCoveringPredicted,trainingData$bugCovering, mode="prec_recall", positive="TRUE")
 

```
<p>Compute the minimal ranking value that corresponded to the predicted bugCovering questions
```{r knn.caret.ranking, echo=FALSE}
df<-data.frame(bugCoveringPredicted)
predictedBugCoveringList<-trainingData[df[,1]==TRUE,];
rankingList <- as.numeric(unlist(predictedBugCoveringList[,2]));
mean(rankingList)
max(rankingList)
hist(rankingList,main="Bug-covering ranking dist., knn caret repeatedcv, mean=1.52, max=2",xlab="ranking by number of YES's");
```

<p>By the distribution of ranking outcomes, we can note that the metric value for Ranking vote has to be larger or equal to 3 (three) in order predict bug-covering questions.</p>

<p></p>

## Conclusions
<p>Caret produced more false positives than knn.cv from class package. I tried to fine tune it more, but was not enough. The k value for Caret seems very high 15 t 23. However, these differences might be due to my use of knn.cv to be overfitting.</p>

<br><br>
